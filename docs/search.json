[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ruoqi Li",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nRuoqi Li\n\n\nMay 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYour Name\n\n\nMay 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nRuoqi Li\n\n\nMay 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nRuoqi Li\n\n\nMay 22, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter.\nEach recipient was randomly assigned to receive one of the following treatments:\n\nA standard solicitation letter (control group)\nA letter including a matching donation offer (treatment group)\nVariations within the matching group that differed by:\n\nMatch ratio (1:1, 2:1, or 3:1)\nMaximum match amount ($25k, $50k, $100k, or unstated)\nSuggested donation level (e.g., base amount, 1.25x, or 1.5x of prior donation)\n\n\nThe matching grant was framed as coming from a “concerned fellow member” who would match donations received up to a certain amount, providing a perceived multiplier effect to the recipient’s contribution.\nThis design allowed the authors to explore not only the overall impact of offering a match on donation rates and amounts, but also to test whether the size of the match ratio or the suggested donation amount affected donor behavior.\nThey published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#introduction",
    "href": "projects/project2/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter.\nEach recipient was randomly assigned to receive one of the following treatments:\n\nA standard solicitation letter (control group)\nA letter including a matching donation offer (treatment group)\nVariations within the matching group that differed by:\n\nMatch ratio (1:1, 2:1, or 3:1)\nMaximum match amount ($25k, $50k, $100k, or unstated)\nSuggested donation level (e.g., base amount, 1.25x, or 1.5x of prior donation)\n\n\nThe matching grant was framed as coming from a “concerned fellow member” who would match donations received up to a certain amount, providing a perceived multiplier effect to the recipient’s contribution.\nThis design allowed the authors to explore not only the overall impact of offering a match on donation rates and amounts, but also to test whether the size of the match ratio or the suggested donation amount affected donor behavior.\nThey published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#data",
    "href": "projects/project2/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n1. mrm2 (Months Since Last Donation)\nWe first test whether the treatment and control groups differ significantly in the number of months since their last donation (mrm2). This helps verify whether the randomization process created balanced groups.\n\n\n(0.0764041463527638, 0.9390978830677066)\n\n\n\nThe t-test returns a t-statistic of 0.076 and a p-value of 0.939, indicating no statistically significant difference in the mrm2 variable between the two groups.\n\nWe also run a regression of mrm2 ~ treatment to confirm the result:\n\nreg_model = smf.ols('mrm2 ~ treatment', data=df_clean).fit()\nreg_model.params.to_frame(\"coef\").join(reg_model.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n12.984577\n0.000000\n\n\ntreatment\n0.008835\n0.939098\n\n\n\n\n\n\n\n\nThe estimated coefficient on treatment is 0.0088 with a p-value of 0.939, again showing no meaningful difference.\n\n\n\nSummary statistics:\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n0\n12.984577\n12.065892\n16339\n\n\n1\n12.993411\n12.064867\n32632\n\n\n\n\n\n\n\nThe table above shows the average number of months since the last donation (mrm2) for each group:\n\nControl group (treatment = 0): 12.98 months\nTreatment group (treatment = 1): 12.99 months\n\nThe means are nearly identical, differing by only 0.0088 months.\nThis minimal difference provides strong visual evidence that the groups are balanced on this pre-treatment variable.\n\n\nSummary Interpretation:\n\nT-test result: t = 0.076, p = 0.939 → No significant difference\nRegression: treatment coefficient is 0.0088, also not significant\nConclusion: There is no statistical evidence of a difference in the time since last donation between the treatment and control groups.\n\n\n\n2. years (Years Since First Donation)\nWe now test whether the treatment and control groups differ significantly in the number of years since the individual first donated (years). This variable serves as a proxy for donor loyalty and tenure.\n\n\n(-1.1939644099545728, 0.23249767383190834)\n\n\n\nThe t-test returns a t-statistic of -1.194 and a p-value of 0.232, indicating no statistically significant difference in the years variable between the treatment and control groups. This suggests that both groups had similar donation histories prior to the experiment.\n\nWe also run a regression of years ~ treatment to confirm the result:\n\nreg_model = smf.ols('years ~ treatment', data=df_clean).fit()\nreg_model.params.to_frame(\"coef\").join(reg_model.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n6.140645\n0.000000\n\n\ntreatment\n-0.063114\n0.232498\n\n\n\n\n\n\n\n\nThe estimated coefficient on treatment is -0.0631 with a p-value of 0.232, again showing no meaningful difference.\n\nThese results support the conclusion that the randomization procedure successfully balanced the groups in terms of how long individuals had been donating to the organization.\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n0\n6.140645\n5.636459\n16339\n\n\n1\n6.077531\n5.454186\n32632\n\n\n\n\n\n\n\nThe table above shows the average number of years since the first donation (years) for each group:\n\nControl group (treatment = 0): 6.14 years\nTreatment group (treatment = 1): 6.08 years\n\nThe means are nearly identical, differing by only 0.0631 years (approximately 23 days).\nThis minimal difference provides strong visual evidence that the groups are balanced on this pre-treatment variable, reinforcing the validity of the random assignment.\n\n\nSummary Interpretation:\n\nT-test result: t = -1.194, p = 0.232 → No significant difference\nRegression: treatment coefficient = -0.0631, also not significant\nConclusion: There is no statistical evidence of a difference in donor tenure between the treatment and control groups.\n\nThese results confirm that the groups are balanced in terms of donor tenure.\nThe near-identical averages and the lack of statistically significant difference indicate that the treatment assignment did not disrupt the pre-existing characteristics of the sample.\nThis supports the internal validity of the experimental design.\n\n\n3. freq (Number of Prior Donations)\nWe now test whether the treatment and control groups are balanced in terms of their prior donation frequency (freq). This variable captures how often individuals have previously given and reflects general giving propensity.\n\n\n(-0.059722457211553545, 0.9523769290955975)\n\n\n\nThe t-test returns a t-statistic of -0.060 and a p-value of 0.952, indicating no statistically significant difference in the freq variable between the treatment and control groups.\n\nWe also run a regression of freq ~ treatment to confirm the result:\n\nreg_model = smf.ols('freq ~ treatment', data=df_clean).fit()\nreg_model.params.to_frame(\"coef\").join(reg_model.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n8.068119\n0.000000\n\n\ntreatment\n-0.006554\n0.952377\n\n\n\n\n\n\n\n\nThe estimated coefficient on treatment is -0.0066 with a p-value of 0.952, again showing no meaningful difference.\n\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n0\n8.068119\n11.449873\n16339\n\n\n1\n8.061565\n11.450896\n32632\n\n\n\n\n\n\n\nThe table above shows the average number of prior donations (freq) for each group:\n\nControl group (treatment = 0): 8.07 donations\nTreatment group (treatment = 1): 8.06 donations\n\nThe means are nearly identical, differing by only 0.0066 donations.\nThis minimal difference provides strong visual and statistical evidence that the groups are balanced on this pre-treatment variable.\n\n\nSummary Interpretation:\n\nT-test result: t = -0.060, p = 0.952 → No significant difference\nRegression: treatment coefficient = -0.0066, p = 0.952 → also not significant\nConclusion: There is no statistical evidence of a difference in prior donation frequency between the groups."
  },
  {
    "objectID": "projects/project2/index.html#experimental-results",
    "href": "projects/project2/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. And begin by examining whether the treatment group (those who received a matching donation offer) had a higher probability of making a charitable contribution (gave = 1) than the control group.\n\n\n\n\n\n\n\n\n\ntreatment\ngave\n\n\n\n\n0\nControl\n0.017858\n\n\n1\nTreatment\n0.022039\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar plot shows a higher donation rate in the treatment group compared to the control group.\nThis suggests a potential positive effect of matching donations on participation in giving.\n\nI also conduct a t-test to assess whether the difference in donation rates is statistically significant.\n\n\n(3.1013610005439456, 0.0019274025949016982)\n\n\n\nThe t-test returns a t-statistic of 3.10 and a p-value of 0.0019, indicating a statistically significant difference between the groups.\nThis result suggests that the matching donation treatment had a positive effect on the likelihood of giving.\n\nTo further investigate the effect of the matching donation offer on donation behavior, we run a simple linear regression where the dependent variable is gave (1 if donated, 0 if not) and the independent variable is treatment (1 = received match offer, 0 = control).\n\nols_model = smf.ols('gave ~ treatment', data=df_contribution).fit()\nols_model.params.to_frame(\"coef\").join(ols_model.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n0.017858\n4.779032e-59\n\n\ntreatment\n0.004180\n1.927403e-03\n\n\n\n\n\n\n\n\nThe OLS regression estimates that the coefficient on treatment is 0.0042, with a p-value of 0.002.\nThis means that being in the treatment group increased the probability of donation by about 0.42 percentage points, and this effect is statistically significant.\n\nThis result is consistent with the earlier t-test, which also showed a significant difference between groups.\nTo more accurately model the binary outcome variable (gave), we also estimate a Probit regression. This approach accounts for the 0/1 nature of the dependent variable and models the probability that an individual donates.\n\nprobit_model = smf.probit('gave ~ treatment', data=df_contribution).fit()\nprobit_model.params.to_frame(\"coef\").join(probit_model.pvalues.to_frame(\"p-value\"))\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n-2.100141\n0.000000\n\n\ntreatment\n0.086785\n0.001852\n\n\n\n\n\n\n\n\nThe estimated coefficient on treatment is 0.0868, with a p-value of 0.002.\nThis indicates that being assigned to the treatment group significantly increases the probability of donating.\n\nAlthough the Probit coefficient is not directly interpretable as a marginal effect (like in OLS), the positive sign and low p-value confirm that the treatment effect is positive and statistically significant.\n\nSummary: Charitable Contribution Made\nIn this section, I analyzed whether offering a matching donation increases the likelihood that individuals donate.\n\nI first compared donation rates between the treatment and control groups using a bar plot. The visual comparison showed a modest increase in the treatment group.\nA t-test confirmed that this difference is statistically significant (t = 3.10, p = 0.0019), suggesting that the treatment effect is not due to random chance.\nI then estimated an OLS regression of gave ~ treatment, which indicated that being in the treatment group increased donation probability by approximately 0.42 percentage points—a meaningful and significant effect.\nFinally, I fit a Probit regression to better model the binary outcome variable. The coefficient on treatment was positive and statistically significant (0.087, p = 0.002), further confirming the effect.\n\nTogether, these results provide strong evidence that matching donation offers increase the probability of giving.\nNOTE: Linear regression results appear replicate Table 3 column 1 in the paper. Probit results do not, despite Table 3 indicating its results come from probit regressions…\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. To test whether higher match ratios (2:1 and 3:1) are more effective than the baseline 1:1 ratio at encouraging charitable contributions.\n\n\n(Ttest_indResult(statistic=0.96504713432247, pvalue=0.33453168549723933),\n Ttest_indResult(statistic=0.05011583793874515, pvalue=0.9600305283739325))\n\n\nThe two-sample t-tests show:\n\n2:1 vs 1:1: t = 0.965, p = 0.335 → not significant\n3:1 vs 2:1: t = 0.050, p = 0.960 → not significant\n\nThese results indicate that there is no statistically significant difference in donation rates between the 1:1 and 2:1 match groups, or between the 2:1 and 3:1 match groups.\n\n\nratio\n1    0.020749\n2    0.022633\n3    0.022733\nName: gave, dtype: float64\n\n\nThe table below shows the average donation rate (gave) by match ratio:\n\n1:1 → 2.07%\n2:1 → 2.26%\n3:1 → 2.27%\n\nWhile donation rates increase slightly as the match ratio rises, the differences are minimal and may not be practically or statistically meaningful.\n\nreg_model_categorical = smf.ols('gave ~ C(ratio)', data=df_match).fit()\nreg_model_categorical.params.to_frame(\"coef\").join(reg_model_categorical.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n0.020749\n3.981333e-50\n\n\nC(ratio)[T.2]\n0.001884\n3.382805e-01\n\n\nC(ratio)[T.3]\n0.001984\n3.133172e-01\n\n\n\n\n\n\n\n\nWe regress gave on the categorical variable ratio, treating 1:1 as the baseline:\n\n\n\nThe intercept (0.0207) represents the donation rate for the 1:1 match group (2.07%)\nThe coefficient on 2:1 is 0.0019 (p = 0.338), indicating a small and not statistically significant increase over the 1:1 group\nThe coefficient on 3:1 is effectively 0 (p = 0.313), indicating no difference compared to the 2:1 group\n\n\nOverall, the regression confirms the t-test findings: higher match ratios do not significantly increase the probability of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\nSize of Charitable Contribution\nIn this section, we examine whether the matching donation treatment affects not only the likelihood of donating, but also the amount donated.\n\n\n1. Analysis on All Observations\nI begin by analyzing the donation amount for the entire sample, including individuals who did not donate (whose amount = 0).\n\n\n(1.860501983704084, 0.06282039496922609)\n\n\n\nThe t-test yields t = 1.86 and p = 0.063, suggesting a marginally non-significant difference.\n\n\nreg_all = smf.ols('amount ~ treatment', data=df_amount).fit()\nreg_all.params.to_frame(\"coef\").join(reg_all.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n0.813268\n1.843438e-33\n\n\ntreatment\n0.153605\n6.282029e-02\n\n\n\n\n\n\n\n\nThe regression estimates a treatment effect of $0.15, with a p-value of 0.063.\nThis suggests that receiving a matching offer slightly increases the average donation, but the effect is not statistically significant at the 5% level.\n\n\n\n\n2. Analysis Conditional on Giving\nNext, I restrict the sample to only those who donated (gave == 1) and repeat the analysis.\n\n\n(-0.5808388621715499, 0.5614758777919849)\n\n\n\nAmong donors only, the t-test show a t-statistic of -0.581 and a p-value of 0.561, indicating no statistically significant difference in the donation amounts between the treatment and control groups.\n\n\nreg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\nreg_pos.params.to_frame(\"coef\").join(reg_pos.pvalues.to_frame(\"p-value\"))\n\n\n\n\n\n\n\n\ncoef\np-value\n\n\n\n\nIntercept\n45.540268\n5.473578e-68\n\n\ntreatment\n-1.668393\n5.614756e-01\n\n\n\n\n\n\n\n\nThe regression shows that treatment group donors gave $1.67 less on average than control group donors, but this difference is not statistically significant.\n\n\n\n\n3. Visualization: Donation Amount Distribution\nI now visualize the donation amounts among those who gave, using histograms for the control and treatment groups.\n\n\n\n\n\n\n\n\n\n\nThe control group’s average donation is around $45.54, while the treatment group’s average is $43.87. Visually, the difference is minimal and statistically insignificant.\n\n\nConclusion:\nThere is no strong evidence that receiving a matching donation offer affects the amount given.\nWhile the treatment may slightly increase overall donation amounts in the full sample, it does not lead to significantly larger gifts among those who do donate."
  },
  {
    "objectID": "projects/project2/index.html#simulation-experiment",
    "href": "projects/project2/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThis simulation illustrates the concept of sampling error—the difference between the sample average and the true population average due to random variation in finite samples.\nBy simulating 10,000 donation outcomes from both the control (p = 0.018) and treatment (p = 0.022) groups, we compute the difference between each pair of observations.\nI plot the cumulative average of these differences as the number of samples increases.\n\nimport numpy as np\n\n# Simulation parameters\np_control = 0.018\np_treatment = 0.022\nn_sim = 10000\n\n# Law of Large Numbers\ncontrol_draws = np.random.binomial(1, p_control, n_sim)\ntreatment_draws = np.random.binomial(1, p_treatment, n_sim)\ndiffs = treatment_draws - control_draws\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_sim + 1)\n\n# Plot cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Treatment Effect (0.004)')\nplt.title('Law of Large Numbers: Cumulative Average of Differences')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that the cumulative average difference between treatment and control groups initially fluctuates but gradually stabilizes around 0.004, the true treatment effect.\nThis demonstrates the Law of Large Numbers: as sample size increases, the sample average converges to the population average.\n\n\n\nCentral Limit Theorem (CLT)\nTo illustrate the Central Limit Theorem, I simulate 1,000 average donation differences between treatment and control groups at different sample sizes: 50, 200, 500, and 1000. The goal is to demonstrate how the sampling distribution of the mean becomes more normal and more concentrated as sample size increases.\nIn this simulation, I repeat the following process 1,000 times for each sample size:\n\nDraw n binary donation outcomes from the control group (p = 0.018)\nDraw n binary donation outcomes from the treatment group (p = 0.022)\nCompute the mean difference between the two samples\nStore and plot the distribution of these mean differences\n\n\nsample_sizes = [50, 200, 500, 1000]\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.flatten()\n\nfor i, size in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, p_control, size)\n        treatment_sample = np.random.binomial(1, p_treatment, size)\n        diff = np.mean(treatment_sample) - np.mean(control_sample)\n        mean_diffs.append(diff)\n    axes[i].hist(mean_diffs, edgecolor='black')\n    axes[i].axvline(x=p_treatment - p_control, color='red', linestyle='--', label='True Effect (0.004)')\n    axes[i].set_title(f'Sample Size = {size}')\n    axes[i].set_xlabel('Mean Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n    axes[i].grid(True)\n\nplt.suptitle('Central Limit Theorem: Sampling Distributions of Mean Differences')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n\n\n\n\n\n\n\n\nAs the sample size increases, the distribution of mean differences becomes: - More symmetric - More tightly centered - Closer to a bell-shaped (normal) distribution\nThis is a visual demonstration of the Central Limit Theorem:\nRegardless of the underlying distribution, the sampling distribution of the sample mean approaches normality as sample size increases.\nBy the time n = 1000, the distribution is sharply centered around the true treatment effect (0.004), confirming that larger samples reduce variability and improve the precision of statistical inference.\nAlso, when sample size ( n &gt; 30 ), the sampling distribution of the mean is typically close enough to normal for t-tests and confidence intervals to be reliable."
  },
  {
    "objectID": "projects/project2/hw1_code.html",
    "href": "projects/project2/hw1_code.html",
    "title": "Balance Test",
    "section": "",
    "text": "import pandas as pd\n\n# Load the Stata dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Display basic information and the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Drop rows with missing values in mrm2 and treatment\ndf_clean = df[['treatment', 'mrm2', 'years', 'freq', 'female']].dropna()\n\n# T-test: compare mrm2 (months since last donation) between treatment and control groups\ntreat_group = df_clean[df_clean['treatment'] == 1]['mrm2']\ncontrol_group = df_clean[df_clean['treatment'] == 0]['mrm2']\nt_stat, p_val = stats.ttest_ind(treat_group, control_group)\n\n# Linear regression: regress mrm2 on treatment\nreg_model = smf.ols('mrm2 ~ treatment', data=df_clean).fit()\nreg_summary = reg_model.summary()\n\n# Collect means and stds for reporting\nmeans = df_clean.groupby('treatment')['mrm2'].agg(['mean', 'std', 'count'])\n\n(t_stat, p_val)\n\n(0.0764041463527638, 0.9390978830677066)\nmeans\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\ntreatment\n\n\n\n\n\n\n\n0\n12.984577\n12.065892\n16339\n\n\n1\n12.993411\n12.064867\n32632\nreg_summary\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmrm2\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.005838\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.939\n\n\nTime:\n13:20:23\nLog-Likelihood:\n-1.9144e+05\n\n\nNo. Observations:\n48971\nAIC:\n3.829e+05\n\n\nDf Residuals:\n48969\nBIC:\n3.829e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.9846\n0.094\n137.564\n0.000\n12.800\n13.170\n\n\ntreatment\n0.0088\n0.116\n0.076\n0.939\n-0.218\n0.235\n\n\n\n\n\n\n\n\nOmnibus:\n7823.434\nDurbin-Watson:\n2.000\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12126.494\n\n\nSkew:\n1.161\nProb(JB):\n0.00\n\n\nKurtosis:\n3.741\nCond. No.\n3.22\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "projects/project2/hw1_code.html#charitable-contribution-made",
    "href": "projects/project2/hw1_code.html#charitable-contribution-made",
    "title": "Balance Test",
    "section": "Charitable Contribution Made",
    "text": "Charitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\n\n# Reload libraries in case of session reset\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n\n# Step 1: Clean data for this analysis\ndf_contribution = df[['treatment', 'gave']].dropna()\n\n# Step 2: Compute donation rates\ndonation_rates = df_contribution.groupby('treatment')['gave'].mean().reset_index()\ndonation_rates['treatment'] = donation_rates['treatment'].map({0: 'Control', 1: 'Treatment'})\n\n# Step 3: Bar plot\nplt.figure(figsize=(6, 4))\nsns.barplot(x='treatment', y='gave', data=donation_rates)\nplt.title('Proportion of People Who Donated')\nplt.ylabel('Donation Rate')\nplt.xlabel('Group')\nplt.ylim(0, 0.03)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar plot shows a higher donation rate in the treatment group.\n\ndonation_rates\n\n\n\n\n\n\n\n\ntreatment\ngave\n\n\n\n\n0\nControl\n0.017858\n\n\n1\nTreatment\n0.022039\n\n\n\n\n\n\n\n\ncontrol_group = df_contribution[df_contribution['treatment'] == 0]['gave']\ntreatment_group = df_contribution[df_contribution['treatment'] == 1]['gave']\nt_stat, p_val = stats.ttest_ind(treatment_group, control_group)\nt_stat, p_val\n\n(3.1013610005439456, 0.0019274025949016982)\n\n\n\n\nA t-test confirms that this difference is statistically significant (p = 0.0019). Linear regression estimates that receiving the match offer increases donation likelihood by 0.42 percentage points.\n\nols_model = smf.ols('gave ~ treatment', data=df_contribution).fit()\nols_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n9.618\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.00193\n\n\nTime:\n13:20:25\nLog-Likelihood:\n26630.\n\n\nNo. Observations:\n50083\nAIC:\n-5.326e+04\n\n\nDf Residuals:\n50081\nBIC:\n-5.324e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\ntreatment\n0.0042\n0.001\n3.101\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\n\nOmnibus:\n59814.280\nDurbin-Watson:\n2.005\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4317152.727\n\n\nSkew:\n6.740\nProb(JB):\n0.00\n\n\nKurtosis:\n46.440\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nUnderstanding the OLS Regression Output\nThe OLS regression estimates the following model:\ngave ~ treatment\nWhere: - gave is a binary variable indicating whether someone donated (1 = yes, 0 = no) - treatment is 1 if the individual received a matching donation offer, and 0 otherwise\nIn this model: - The intercept represents the average donation rate in the control group - The coefficient on treatment represents the difference in donation rates between the treatment and control groups\nFrom our regression: - Intercept = 0.0179 → This matches the donation rate in the control group: 1.79% - Treatment coefficient = 0.0042 → This means the treatment group donated 0.42 percentage points more than the control group\nThese numbers exactly match our earlier manual calculations: - Control group donation rate: 0.017858 - Treatment group donation rate: 0.022039 - Difference: 0.004181\nThus, the OLS regression provides an alternative but equivalent way of estimating the average treatment effect (ATE) in this binary outcome context.\n\nprobit_model = smf.probit('gave ~ treatment', data=df_contribution).fit()\nprobit_model.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nWed, 23 Apr 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n13:20:25\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nThe probit regression confirms that assignment to the match treatment significantly increased the probability of donating. The coefficient on treatment is positive (0.0868) and statistically significant (p = 0.002), suggesting that matching appeals have a real behavioral impact on charitable decisions."
  },
  {
    "objectID": "projects/project2/hw1_code.html#differences-between-match-rates",
    "href": "projects/project2/hw1_code.html#differences-between-match-rates",
    "title": "Balance Test",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\n\nIn this section, I examine whether higher match ratios (2:1 or 3:1) lead to a higher probability of donation compared to the baseline 1:1 match.\n\n# group 1 2 3\ndf['ratio'] = df['ratio'].astype(str)\ndf_match = df[(df['treatment'] == 1) & (df['ratio'].isin(['1', '2', '3']))].copy()\n\ndf_match['ratio1'] = (df_match['ratio'] == '1').astype(int)\ndf_match['ratio2'] = (df_match['ratio'] == '2').astype(int)\ndf_match['ratio3'] = (df_match['ratio'] == '3').astype(int)\n\n# Run t-tests comparing donation rates between different ratios\nrate_1 = df_match[df_match['ratio1'] == 1]['gave']\nrate_2 = df_match[df_match['ratio2'] == 1]['gave']\nrate_3 = df_match[df_match['ratio3'] == 1]['gave']\n\nt_12 = stats.ttest_ind(rate_2, rate_1)\nt_23 = stats.ttest_ind(rate_3, rate_2)\nt_12, t_23\n\n(Ttest_indResult(statistic=0.96504713432247, pvalue=0.33453168549723933),\n Ttest_indResult(statistic=0.05011583793874515, pvalue=0.9600305283739325))\n\n\nWe conduct two-sample t-tests to compare the response rates across different match ratios:\n\n2:1 vs 1:1\n\nt = 0.965, p = 0.334 → not statistically significant\n\n3:1 vs 2:1\n\nt = 0.050, p = 0.960 → not statistically significant\n\n\nThese tests suggest that while donation rates rise slightly with higher match ratios, the differences are not statistically meaningful.\n\n# Step 5: Compute donation rate means for each ratio\ndonation_means = df_match.groupby('ratio')['gave'].mean()\ndonation_means\n\nratio\n1    0.020749\n2    0.022633\n3    0.022733\nName: gave, dtype: float64\n\n\n\n\nThe donation rate increases slightly as the match ratio increases. However, these differences are relatively small in magnitude.\n\nreg_model_categorical = smf.ols('gave ~ C(ratio)', data=df_match).fit()\nreg_model_categorical.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 23 Apr 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n13:20:25\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nC(ratio)[T.2]\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nC(ratio)[T.3]\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nNone of the differences are statistically significant at the 5% level. This further supports the conclusion that larger match ratios do not significantly improve the likelihood of donating."
  },
  {
    "objectID": "projects/project3/hw2_questions.html",
    "href": "projects/project3/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nThe histogram shows that firms who are Blueprinty customers tend to have slightly more patents than non-customers.\nThe average number of patents over the last 5 years is higher for customers about 4.13 compared to non-customers about 3.47.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\nBlueprinty customers are not evenly distributed across regions. Some regions, such as the Midwest and Southwest, have more customers than others. Location may influence whether a firm adopts Blueprinty’s software. In terms of age, customers are have similar age which is 26.9 years compared to non-customers 26.1 years.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample of ( n ) observations ( Y_1, Y_2, …, Y_n ),\nthe likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood becomes: \\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\n\\]\nIn our implementation, we omit the constant ( Y_i! ) since it does not affect the maximization.\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda)\n\n\n\n\n\n\n\n\n\n\nThe plot above shows the Poisson log-likelihood function evaluated at a range of possible values for lambda,\nusing the observed number of patents as data.\n\nThe log-likelihood curve is concave, peaking at the value of lambda that best explains the data.\nThis peak corresponds to the maximum likelihood estimate (MLE) of lambda, which in the case of the Poisson distribution is the sample mean of Y.\n\n\n\n\nWe take the first derivative of the log-likelihood function, set it equal to zero, and solve for lambda.\nRecall the log-likelihood function (ignoring constants):\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda \\right)\n\\]\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( \\frac{Y_i}{\\lambda} - 1 \\right)\n= \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n\n\\]\nSet the derivative equal to zero and solve:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n = 0\n\\]\n\\[\n\\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result confirms that the maximum likelihood estimator for lambda in a Poisson model is simply the sample mean of the data.\nThis aligns with our intuition, as the Poisson distribution assumes the mean and variance are both equal to lambda.\n\n\n\n\ndef neg_loglik(lmbda):\n    if lmbda[0] &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lmbda[0]) - lmbda[0])\n\n# Use scipy.optimize.minimize to find MLE\nresult = optimize.minimize(neg_loglik, x0=[1.0], bounds=[(0.01, None)])\nlambda_mle_optim = result.x[0]\nlambda_mle_optim\n\n3.6846667444219783\n\n\n\n\nThe code above uses scipy.optimize.minimize to numerically maximize the Poisson log-likelihood function by minimizing its negative.\n\nThe objective function is written in terms of lambda, the rate parameter of the Poisson distribution.\nThe optimizer searches for the value of lambda that best fits the observed patent data.\n\nThe result shows that the MLE for lambda is approximately 3.68,\nwhich matches the sample mean of the data which the analytical MLE we derived earlier. And this also matched with previous graph.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\ndef poisson_regression_loglik(beta, Y, X):\n    \"\"\"\n    beta: coefficient vector\n    Y: observed count outcome (n,)\n    X: covariate matrix (n, k)\n    \"\"\"\n    lambda_i = np.exp(X @ beta) \n    loglik = np.sum(Y * np.log(lambda_i) - lambda_i)\n    return loglik\n\n\n\n\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\cdot \\text{region}_{NE} + \\beta_2 \\cdot \\text{region}_{NW} + \\cdots + \\beta_k \\cdot \\text{iscust}\n\\]\nHere, \\(\\lambda_i\\) represents the expected number of patents for firm \\(i\\),\nwhich is obtained through the inverse link function: exp(X @ beta).\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numdifftools as nd\ndf = df.dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\ndf[\"age_c\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_c_sq\"] = df[\"age_c\"] ** 2\n\n\n\n\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"reg\", drop_first=True)\ndesign_matrix = pd.concat([\n    pd.Series(1, name=\"const\", index=df.index),\n    df[[\"age_c\", \"age_c_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_vals = design_matrix.to_numpy(dtype=float)\ny_vals = df[\"patents\"].to_numpy(dtype=float)\n\n\n\n\n\ndef log_likelihood_poisson(params, y, X):\n    eta = X @ params\n    mu = np.exp(eta)\n    return np.sum(y * np.log(mu) - mu - gammaln(y + 1))\n\ndef neg_loglik(params):\n    return -log_likelihood_poisson(params, y_vals, X_vals)\n\n\n\n\n\ninit_params = np.zeros(X_vals.shape[1])\nopt = minimize(neg_loglik, init_params, method=\"BFGS\")\ncoef_est = opt.x\n\n\n\n\n\nhess_calc = nd.Hessian(neg_loglik)\nhess_matrix = hess_calc(coef_est)\ncov_est = np.linalg.inv(hess_matrix)\nstd_errors = np.sqrt(np.diag(cov_est))\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nStdError\n\n\n\n\n0\nconst\n1.344676\n0.038311\n\n\n1\nage_c\n-0.007970\n0.002074\n\n\n2\nage_c_sq\n-0.002970\n0.000254\n\n\n3\niscustomer\n0.207591\n0.030892\n\n\n4\nreg_Northeast\n0.029170\n0.043624\n\n\n5\nreg_Northwest\n-0.017575\n0.053781\n\n\n6\nreg_South\n0.056561\n0.052662\n\n\n7\nreg_Southwest\n0.050576\n0.047198\n\n\n\n\n\n\n\n\n\n\n\nimport patsy\ndf[\"age_sq\"] = df[\"age\"] ** 2\nformula = \"patents ~ age + age_sq + C(region) + iscustomer\"\ny, X = patsy.dmatrices(formula, df, return_type='dataframe')\nY = np.asarray(y).flatten()\n\nimport statsmodels.api as sm\n# Fit Poisson GLM using statsmodels\nglm_poisson = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\n\n# Extract coefficients and standard errors\nsummary_glm = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.columns)\nsummary_glm\n\n\n\n\n\n\n\n\nCoefficient (GLM)\nStd. Error (GLM)\n\n\n\n\nIntercept\n-0.508920\n0.183179\n\n\nC(region)[T.Northeast]\n0.029170\n0.043625\n\n\nC(region)[T.Northwest]\n-0.017575\n0.053781\n\n\nC(region)[T.South]\n0.056561\n0.052662\n\n\nC(region)[T.Southwest]\n0.050576\n0.047198\n\n\nage\n0.148619\n0.013869\n\n\nage_sq\n-0.002970\n0.000258\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\nIdentical coefficient estimates and standard errors which confirming that our custom approach is implemented correctly.\n\n\n\n\nThe estimated intercept is 1.3447, representing the expected log number of patents for the reference group: individuals in the base region, with centered age = 0 (i.e., average age), and not customers.\nThe corresponding expected number of patents is exp(1.3447) ≈ 3.84.\n\n\n\n\n\nage_c has a negative coefficient (-0.0080), suggesting that as age increases (from the mean), the expected number of patents decreases slightly.\nage_c_sq is also negative, indicating a concave (inverted U-shaped) relationship between age and patents — i.e., patent activity rises with age to a point and then falls.\n\n\n\n\n\nThe coefficient of 0.2076 implies that customers have higher expected patent counts than non-customers.\nInterpreted as an incidence rate ratio: exp(0.2076) ≈ 1.23, meaning customers have about 23% more expected patents than non-customers, holding other factors constant.\n\n\n\n\n\nDummy variables for reg_Northeast, reg_Northwest, reg_South, and reg_Southwest reflect differences from the reference region.\nThese coefficients are relatively small in magnitude and have larger standard errors, suggesting that regional differences may not be statistically significant in explaining patent counts.\n\n\n\n\n\n\nX_0 = X.copy()\nX_1 = X.copy()\niscust_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n# Set iscustomer = 0 in X_0, 1 in X_1\nX_0[iscust_col] = 0\nX_1[iscust_col] = 1\nX_0_np = np.asarray(X_0)\nX_1_np = np.asarray(X_1)\n\n# Get predicted patent counts\ny_pred_0 = np.exp(X_0_np @ glm_results.params)\ny_pred_1 = np.exp(X_1_np @ glm_results.params)\n\n#Uplift\nuplift = y_pred_1 - y_pred_0\naverage_uplift = np.mean(uplift)\naverage_uplift\n\n0.7927680710453031\n\n\n\nFirms using Blueprinty are 0.793 more patents over 5 years compared to if they had not used the software."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "href": "projects/project3/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nThe histogram shows that firms who are Blueprinty customers tend to have slightly more patents than non-customers.\nThe average number of patents over the last 5 years is higher for customers about 4.13 compared to non-customers about 3.47.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\nBlueprinty customers are not evenly distributed across regions. Some regions, such as the Midwest and Southwest, have more customers than others. Location may influence whether a firm adopts Blueprinty’s software. In terms of age, customers are have similar age which is 26.9 years compared to non-customers 26.1 years.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe assume:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample of ( n ) observations ( Y_1, Y_2, …, Y_n ),\nthe likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood becomes: \\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\n\\]\nIn our implementation, we omit the constant ( Y_i! ) since it does not affect the maximization.\n\n\n\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda)\n\n\n\n\n\n\n\n\n\n\nThe plot above shows the Poisson log-likelihood function evaluated at a range of possible values for lambda,\nusing the observed number of patents as data.\n\nThe log-likelihood curve is concave, peaking at the value of lambda that best explains the data.\nThis peak corresponds to the maximum likelihood estimate (MLE) of lambda, which in the case of the Poisson distribution is the sample mean of Y.\n\n\n\n\nWe take the first derivative of the log-likelihood function, set it equal to zero, and solve for lambda.\nRecall the log-likelihood function (ignoring constants):\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda \\right)\n\\]\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( \\frac{Y_i}{\\lambda} - 1 \\right)\n= \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n\n\\]\nSet the derivative equal to zero and solve:\n\\[\n\\frac{1}{\\lambda} \\sum_{i=1}^n Y_i - n = 0\n\\]\n\\[\n\\Rightarrow \\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result confirms that the maximum likelihood estimator for lambda in a Poisson model is simply the sample mean of the data.\nThis aligns with our intuition, as the Poisson distribution assumes the mean and variance are both equal to lambda.\n\n\n\n\ndef neg_loglik(lmbda):\n    if lmbda[0] &lt;= 0:\n        return np.inf\n    return -np.sum(Y * np.log(lmbda[0]) - lmbda[0])\n\n# Use scipy.optimize.minimize to find MLE\nresult = optimize.minimize(neg_loglik, x0=[1.0], bounds=[(0.01, None)])\nlambda_mle_optim = result.x[0]\nlambda_mle_optim\n\n3.6846667444219783\n\n\n\n\nThe code above uses scipy.optimize.minimize to numerically maximize the Poisson log-likelihood function by minimizing its negative.\n\nThe objective function is written in terms of lambda, the rate parameter of the Poisson distribution.\nThe optimizer searches for the value of lambda that best fits the observed patent data.\n\nThe result shows that the MLE for lambda is approximately 3.68,\nwhich matches the sample mean of the data which the analytical MLE we derived earlier. And this also matched with previous graph.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\ndef poisson_regression_loglik(beta, Y, X):\n    \"\"\"\n    beta: coefficient vector\n    Y: observed count outcome (n,)\n    X: covariate matrix (n, k)\n    \"\"\"\n    lambda_i = np.exp(X @ beta) \n    loglik = np.sum(Y * np.log(lambda_i) - lambda_i)\n    return loglik\n\n\n\n\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\cdot \\text{region}_{NE} + \\beta_2 \\cdot \\text{region}_{NW} + \\cdots + \\beta_k \\cdot \\text{iscust}\n\\]\nHere, \\(\\lambda_i\\) represents the expected number of patents for firm \\(i\\),\nwhich is obtained through the inverse link function: exp(X @ beta).\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport numdifftools as nd\ndf = df.dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\ndf[\"age_c\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_c_sq\"] = df[\"age_c\"] ** 2\n\n\n\n\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"reg\", drop_first=True)\ndesign_matrix = pd.concat([\n    pd.Series(1, name=\"const\", index=df.index),\n    df[[\"age_c\", \"age_c_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\nX_vals = design_matrix.to_numpy(dtype=float)\ny_vals = df[\"patents\"].to_numpy(dtype=float)\n\n\n\n\n\ndef log_likelihood_poisson(params, y, X):\n    eta = X @ params\n    mu = np.exp(eta)\n    return np.sum(y * np.log(mu) - mu - gammaln(y + 1))\n\ndef neg_loglik(params):\n    return -log_likelihood_poisson(params, y_vals, X_vals)\n\n\n\n\n\ninit_params = np.zeros(X_vals.shape[1])\nopt = minimize(neg_loglik, init_params, method=\"BFGS\")\ncoef_est = opt.x\n\n\n\n\n\nhess_calc = nd.Hessian(neg_loglik)\nhess_matrix = hess_calc(coef_est)\ncov_est = np.linalg.inv(hess_matrix)\nstd_errors = np.sqrt(np.diag(cov_est))\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nStdError\n\n\n\n\n0\nconst\n1.344676\n0.038311\n\n\n1\nage_c\n-0.007970\n0.002074\n\n\n2\nage_c_sq\n-0.002970\n0.000254\n\n\n3\niscustomer\n0.207591\n0.030892\n\n\n4\nreg_Northeast\n0.029170\n0.043624\n\n\n5\nreg_Northwest\n-0.017575\n0.053781\n\n\n6\nreg_South\n0.056561\n0.052662\n\n\n7\nreg_Southwest\n0.050576\n0.047198\n\n\n\n\n\n\n\n\n\n\n\nimport patsy\ndf[\"age_sq\"] = df[\"age\"] ** 2\nformula = \"patents ~ age + age_sq + C(region) + iscustomer\"\ny, X = patsy.dmatrices(formula, df, return_type='dataframe')\nY = np.asarray(y).flatten()\n\nimport statsmodels.api as sm\n# Fit Poisson GLM using statsmodels\nglm_poisson = sm.GLM(Y, X, family=sm.families.Poisson())\nglm_results = glm_poisson.fit()\n\n# Extract coefficients and standard errors\nsummary_glm = pd.DataFrame({\n    \"Coefficient (GLM)\": glm_results.params,\n    \"Std. Error (GLM)\": glm_results.bse\n}, index=X.columns)\nsummary_glm\n\n\n\n\n\n\n\n\nCoefficient (GLM)\nStd. Error (GLM)\n\n\n\n\nIntercept\n-0.508920\n0.183179\n\n\nC(region)[T.Northeast]\n0.029170\n0.043625\n\n\nC(region)[T.Northwest]\n-0.017575\n0.053781\n\n\nC(region)[T.South]\n0.056561\n0.052662\n\n\nC(region)[T.Southwest]\n0.050576\n0.047198\n\n\nage\n0.148619\n0.013869\n\n\nage_sq\n-0.002970\n0.000258\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\nIdentical coefficient estimates and standard errors which confirming that our custom approach is implemented correctly.\n\n\n\n\nThe estimated intercept is 1.3447, representing the expected log number of patents for the reference group: individuals in the base region, with centered age = 0 (i.e., average age), and not customers.\nThe corresponding expected number of patents is exp(1.3447) ≈ 3.84.\n\n\n\n\n\nage_c has a negative coefficient (-0.0080), suggesting that as age increases (from the mean), the expected number of patents decreases slightly.\nage_c_sq is also negative, indicating a concave (inverted U-shaped) relationship between age and patents — i.e., patent activity rises with age to a point and then falls.\n\n\n\n\n\nThe coefficient of 0.2076 implies that customers have higher expected patent counts than non-customers.\nInterpreted as an incidence rate ratio: exp(0.2076) ≈ 1.23, meaning customers have about 23% more expected patents than non-customers, holding other factors constant.\n\n\n\n\n\nDummy variables for reg_Northeast, reg_Northwest, reg_South, and reg_Southwest reflect differences from the reference region.\nThese coefficients are relatively small in magnitude and have larger standard errors, suggesting that regional differences may not be statistically significant in explaining patent counts.\n\n\n\n\n\n\nX_0 = X.copy()\nX_1 = X.copy()\niscust_col = [col for col in X.columns if \"iscustomer\" in col][0]\n\n# Set iscustomer = 0 in X_0, 1 in X_1\nX_0[iscust_col] = 0\nX_1[iscust_col] = 1\nX_0_np = np.asarray(X_0)\nX_1_np = np.asarray(X_1)\n\n# Get predicted patent counts\ny_pred_0 = np.exp(X_0_np @ glm_results.params)\ny_pred_1 = np.exp(X_1_np @ glm_results.params)\n\n#Uplift\nuplift = y_pred_1 - y_pred_0\naverage_uplift = np.mean(uplift)\naverage_uplift\n\n0.7927680710453031\n\n\n\nFirms using Blueprinty are 0.793 more patents over 5 years compared to if they had not used the software."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#airbnb-case-study",
    "href": "projects/project3/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nExploratory Data Analysis\n\ndf = pd.read_csv(\"airbnb.csv\")\n# 1. Numeric variable EDA\nnumeric_cols = [\n    \"number_of_reviews\", \"days\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n# Describe numeric variables\nnumeric_summary = df[numeric_cols].describe().T\nnumeric_summary\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnumber_of_reviews\n40628.0\n15.904426\n29.246009\n0.0\n1.0\n4.0\n17.0\n421.0\n\n\ndays\n40628.0\n1102.368219\n1383.269358\n1.0\n542.0\n996.0\n1535.0\n42828.0\n\n\nbathrooms\n40468.0\n1.124592\n0.385884\n0.0\n1.0\n1.0\n1.0\n8.0\n\n\nbedrooms\n40552.0\n1.147046\n0.691746\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n40628.0\n144.760732\n210.657597\n10.0\n70.0\n100.0\n170.0\n10000.0\n\n\nreview_scores_cleanliness\n30433.0\n9.198370\n1.119935\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30374.0\n9.413544\n0.844949\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30372.0\n9.331522\n0.902966\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\n\n# 2. Missing value check\n# Calculate the proportion of missing values\nmissing_summary = df[numeric_cols + [\"room_type\", \"instant_bookable\"]].isna().mean().sort_values(ascending=False)\nmissing_summary\n\nreview_scores_value          0.252437\nreview_scores_location       0.252388\nreview_scores_cleanliness    0.250935\nbathrooms                    0.003938\nbedrooms                     0.001871\nnumber_of_reviews            0.000000\ndays                         0.000000\nprice                        0.000000\nroom_type                    0.000000\ninstant_bookable             0.000000\ndtype: float64\n\n\n\n# 3. Distribution plots (numeric)\nplt.figure(figsize=(6, 4))\nsns.histplot(df[\"number_of_reviews\"].dropna(), bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(6, 4))\nsns.histplot(df[\"price\"].dropna(), bins=50)\nplt.yscale(\"log\")\nplt.title(\"Distribution of Price (Log Scale)\")\nplt.xlabel(\"Price (USD)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of Reviews:\nThe distribution is highly right-skewed. Most listings have very few reviews, with a large spike at zero or single-digit counts.\nOnly a small number of listings receive over 100 reviews. This confirms that review counts are not normally distributed and supports the use of a Poisson model for count data.\nPrice (Log Scale):\nPrices also show a strong right skew, with the majority of listings priced under $500.\nThe log-scale y-axis reveals a long tail of high-priced listings, some reaching $10,000.\nA few peaks in the upper end may indicate outliers or intentionally high-priced luxury listings.\nThis distribution suggests that price might need transformation or binning in further analysis.\n\n\n# 4. Correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df[numeric_cols].corr(), annot=True, cmap=\"Blues\", fmt=\".2f\")\nplt.title(\"Correlation Matrix of Numeric Variables\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMost numeric variables have low correlations with each other, suggesting no strong multicollinearity.\n\n# 5. Categorical variable EDA\n# Frequency + Proportion\nroom_type_counts = df[\"room_type\"].value_counts()\nroom_type_props = df[\"room_type\"].value_counts(normalize=True)\n\ninstant_counts = df[\"instant_bookable\"].value_counts()\ninstant_props = df[\"instant_bookable\"].value_counts(normalize=True)\n\n# Grouped means\nroom_type_review_mean = df.groupby(\"room_type\")[\"number_of_reviews\"].mean()\ninstant_review_mean = df.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean()\n\nroom_summary = pd.DataFrame({\n    \"Count\": room_type_counts,\n    \"Proportion\": room_type_props,\n    \"Avg_Reviews\": room_type_review_mean\n})\n\ninstant_summary = pd.DataFrame({\n    \"Count\": instant_counts,\n    \"Proportion\": instant_props,\n    \"Avg_Reviews\": instant_review_mean\n})\n\nroom_summary\n\n\n\n\n\n\n\n\nCount\nProportion\nAvg_Reviews\n\n\nroom_type\n\n\n\n\n\n\n\nEntire home/apt\n19873\n0.489145\n16.823580\n\n\nPrivate room\n19532\n0.480752\n15.222609\n\n\nShared room\n1223\n0.030102\n11.857727\n\n\n\n\n\n\n\n\ninstant_summary\n\n\n\n\n\n\n\n\nCount\nProportion\nAvg_Reviews\n\n\ninstant_bookable\n\n\n\n\n\n\n\nf\n32759\n0.806316\n14.705882\n\n\nt\n7869\n0.193684\n20.894014\n\n\n\n\n\n\n\n\n# 6. Categorical plots\nsns.countplot(x=\"room_type\", data=df)\nplt.title(\"Room Type Distribution\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nsns.countplot(x=\"instant_bookable\", data=df)\nplt.title(\"Instant Bookable Distribution\")\nplt.xlabel(\"Instant Bookable\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=df)\nplt.yscale(\"log\")\nplt.title(\"Number of Reviews by Room Type\")\nplt.tight_layout()\nplt.show()\n\nsns.boxplot(x=\"instant_bookable\", y=\"number_of_reviews\", data=df)\nplt.yscale(\"log\")\nplt.title(\"Number of Reviews by Instant Bookable Status\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model for Number of Reviews\nI use review count as a proxy for how often a listing is booked, and model it using Poisson regression, because the target variable is a count (i.e., 0, 1, 2, …) First, I cleaned the dataset, removing rows with missing values in relevant columns. Categorical variables such as room_type and instant_bookable are transformed into numerical or dummy variables. Then, I fit a Poisson Generalized Linear Model (GLM) using statsmodels.\nThe output includes estimated coefficients and standard errors, which are used to interpret how each factor influences the expected number of reviews. Overall, this model allows us to understand which listing characteristics are associated with higher booking activity on Airbnb.\n\ncols = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[cols].dropna()  # Drop missing values\n\n# Encode \"t\"/\"f\" to 1/0\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\n\n# Convert categorical into dummy variables\ndf = pd.get_dummies(df, columns=[\"room_type\"], drop_first=True)\n\n# Prepare model input\ny = df[\"number_of_reviews\"].astype(float)\nX = df.drop(columns=[\"number_of_reviews\"])\nX = sm.add_constant(X)  \nX = X.astype(float) \n# Poisson regression model\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nsummary = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse,\n    \"p-value\": results.pvalues\n})\nsummary\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\np-value\n\n\n\n\nconst\n3.498049\n1.609066e-02\n0.000000e+00\n\n\ndays\n0.000051\n3.909218e-07\n0.000000e+00\n\n\nbathrooms\n-0.117704\n3.749225e-03\n2.427557e-216\n\n\nbedrooms\n0.074087\n1.991742e-03\n7.567674e-303\n\n\nprice\n-0.000018\n8.326458e-06\n3.148517e-02\n\n\nreview_scores_cleanliness\n0.113139\n1.496336e-03\n0.000000e+00\n\n\nreview_scores_location\n-0.076899\n1.608903e-03\n0.000000e+00\n\n\nreview_scores_value\n-0.091076\n1.803855e-03\n0.000000e+00\n\n\ninstant_bookable\n0.345850\n2.890138e-03\n0.000000e+00\n\n\nroom_type_Private room\n-0.010536\n2.738448e-03\n1.193451e-04\n\n\nroom_type_Shared room\n-0.246337\n8.619793e-03\n1.259254e-179\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Coefficients\nAll p-values are below 0.05, indicating that all included predictors are statistically significant at the 5% level. The Poisson regression model estimates the expected number of reviews (as a proxy for bookings) for Airbnb listings. Below are interpretations of selected coefficients:\n\ndays: The coefficient is very small but positive (0.00005), indicating that listings active for longer periods tend to accumulate more reviews over time, though the marginal effect is minimal.\nbathrooms: A negative coefficient (–0.118) suggests that, holding all else constant, units with more bathrooms tend to receive fewer reviews. This may be due to larger or more expensive listings being booked less frequently.\nbedrooms: The coefficient (0.074) is positive, implying that listings with more bedrooms tend to receive more reviews. Larger spaces may be more attractive to groups.\nprice: A small negative effect (–0.000018) suggests that higher nightly prices slightly reduce the number of bookings (reviews), which aligns with economic intuition.\nreview_scores_cleanliness: A positive coefficient (0.113) means that listings with higher cleanliness ratings tend to have more reviews, possibly due to better guest experiences.\nreview_scores_location & value: Both are negatively associated with review counts (–0.077 and –0.091). This might indicate that lower-scoring listings get more negative feedback, but caution is needed—these could reflect complex review dynamics.\ninstant_bookable: A strongly positive coefficient (0.346) indicates that listings allowing instant booking receive significantly more reviews. This makes sense, as they reduce friction in the booking process.\nroom_type:\n\nPrivate room: Has a small negative coefficient (–0.011), suggesting slightly fewer reviews compared to the reference category (likely “Entire home/apt.”).\nShared room: Shows a much larger negative coefficient (–0.246), meaning these listings get substantially fewer bookings/reviews on average.\n\n\n\nSummary\nThe results suggest that review counts and booking activity are influenced by cleanliness, pricing, and accessibility features such as instant booking. These insights can help hosts optimize their listings to increase visibility and engagement on the platform."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#compare-histograms-and-means-of-number-of-patents-by-customer-status",
    "href": "projects/project3/hw2_questions.html#compare-histograms-and-means-of-number-of-patents-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Compare histograms and means of number of patents by customer status",
    "text": "Compare histograms and means of number of patents by customer status\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nThe histogram shows that firms who are Blueprinty customers tend to have slightly more patents than non-customers.\nThe average number of patents over the last 5 years is higher for customers about 4.13 compared to non-customers about 3.47.\nThis suggests a positive association between using Blueprinty’s software and the number of patents awarded. However, this is not necessarily causal, as other factors — such as firm age or regional location — might influence both the decision to adopt Blueprinty’s software and a firm’s overall patenting success.\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project3/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "href": "projects/project3/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time,\nwe can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years.\nWe start by estimating a simple Poisson model via Maximum Likelihood.\n\nLikelihood Function\nWe assume:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample of ( n ) observations ( Y_1, Y_2, …, Y_n ),\nthe likelihood function is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood becomes: \\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log Y_i! \\right)\n\\]\nIn our implementation, we omit the constant ( Y_i! ) since it does not affect the maximization.\n\n\n\nPython Code: Log-Likelihood and MLE Estimation\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import optimize\n\n# Load data\nY = df[\"patents\"].dropna().astype(int).values\n\n# Define log-likelihood function (omit log(Y!) term)\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda)\n\n# Plot log-likelihood across a range of lambda\nlambdas = np.linspace(0.1, 10, 200)\nloglik_values = [poisson_loglikelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(6, 4))\nplt.plot(lambdas, loglik_values, color=\"orange\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\n\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/project4/hw3_questions.html",
    "href": "projects/project4/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/project4/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#simulate-conjoint-data",
    "href": "projects/project4/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\n\nn_respondents = 100\nn_tasks = 10\nn_alternatives = 3\n\nbrands = ['Hulu', 'Netflix', 'Prime']\nprices = np.arange(4, 36, 4)  # $4 to $32\nads = [0, 1]  # 0 = no ads, 1 = has ads\n\n# Utility coefficients\nbeta = {\n    'Netflix': 1.0,\n    'Prime': 0.5,\n    'Ads': -0.8,\n    'Price': -0.1\n}\n\nrows = []\n\nfor resp in range(n_respondents):\n    for task in range(n_tasks):\n        alts = []\n        for alt in range(n_alternatives):\n            brand = np.random.choice(brands)\n            ad = np.random.choice(ads)\n            price = np.random.choice(prices)\n            \n            # Indicator vars\n            netflix = 1 if brand == 'Netflix' else 0\n            prime = 1 if brand == 'Prime' else 0\n            \n            # Utility calculation\n            utility = (beta['Netflix'] * netflix +\n                       beta['Prime'] * prime +\n                       beta['Ads'] * ad +\n                       beta['Price'] * price +\n                       np.random.gumbel())\n            \n            alts.append((brand, ad, price, utility))\n        \n        # Select the alternative with highest utility\n        chosen = np.argmax([u for (_, _, _, u) in alts])\n        \n        for idx, (brand, ad, price, utility) in enumerate(alts):\n            rows.append({\n                'Respondent': resp,\n                'Task': task,\n                'Alternative': idx,\n                'Chosen': 1 if idx == chosen else 0,\n                'Brand': brand,\n                'Netflix': 1 if brand == 'Netflix' else 0,\n                'Prime': 1 if brand == 'Prime' else 0,\n                'Ads': ad,\n                'Price': price\n            })\n\ndf = pd.DataFrame(rows)"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "projects/project4/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nConverting the categorical brand variable into two binary indicators: Netflix, Prime (with Hulu as the reference category),\nIncluding indicators for ads (0 = ad-free, 1 = with ads),\nIncluding the monthly price,\nTracking which alternative was chosen by the respondent.\n\n\ndf['Group'] = df['Respondent'] * n_tasks + df['Task']\ndf_prepared = df[['Respondent', 'Task', 'Alternative', 'Group', 'Chosen',\n                  'Netflix', 'Prime', 'Ads', 'Price']]"
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "projects/project4/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nTo estimate the parameters of the MNL model, we first implement the log-likelihood function. Each individual chooses one alternative from a set of three, and the probability of choosing an alternative is modeled using the softmax function derived from the utility specification.\nThe negative log-likelihood function is computed as follows:\n\n# Log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = X @ beta  # linear part\n    log_likelihood = 0\n    for group in unique_groups:\n        mask = groups == group\n        u = utilities[mask]\n        y_group = y[mask]\n        log_prob = u - logsumexp(u)  # log softmax\n        log_likelihood += np.sum(y_group * log_prob)\n    return -log_likelihood  # negative for minimization\n\nAfter defining the log-likelihood function, we use scipy.optimize.minimize() to estimate the model parameters. We apply the BFGS algorithm to find the maximum likelihood estimates (MLEs), and use the inverse Hessian matrix to compute standard errors and 95% confidence intervals for each parameter.\n\n# Initial guess\nbeta_init = np.zeros(X.shape[1])\n\n# Estimate using scipy.optimize\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n# Compute standard errors from Hessian\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% CI\nz = 1.96\nconf_int = np.vstack((beta_hat - z * se, beta_hat + z * se)).T\n\n# Results table\nparams = ['beta_Netflix', 'beta_Prime', 'beta_Ads', 'beta_Price']\nresults_df = pd.DataFrame({\n    'Parameter': params,\n    'Estimate': beta_hat,\n    'Std. Error': se,\n    'CI Lower (95%)': conf_int[:, 0],\n    'CI Upper (95%)': conf_int[:, 1]\n})\nresults_df\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower (95%)\nCI Upper (95%)\n\n\n\n\n0\nbeta_Netflix\n1.036713\n0.025589\n0.986559\n1.086866\n\n\n1\nbeta_Prime\n0.583184\n0.078580\n0.429167\n0.737200\n\n\n2\nbeta_Ads\n-0.810924\n0.342131\n-1.481501\n-0.140347\n\n\n3\nbeta_Price\n-0.096969\n0.005119\n-0.107003\n-0.086936\n\n\n\n\n\n\n\n\nThe interpretation of each coefficient:\n\nbeta_Netflix = 1.04: Respondents strongly prefer Netflix over the baseline category (Hulu). The 95% confidence interval ([0.99, 1.09]) does not include zero, confirming statistical significance.\nbeta_Prime = 0.58: Amazon Prime is also preferred relative to Hulu, though to a lesser extent than Netflix. The effect is statistically significant.\nbeta_Ads = -0.81: There is a strong negative impact of advertisements on utility, as indicated by the large negative estimate and a confidence interval well below zero.\nbeta_Price = -0.097: Price has a small but statistically significant negative effect on utility, consistent with theoretical expectations."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "projects/project4/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nimport numpy as np\ndef log_prior(beta):\n    logp = 0\n    logp += -0.5 * np.sum((beta[:3] / 5) ** 2) - 3 * np.log(5 * np.sqrt(2 * np.pi))\n    logp += -0.5 * (beta[3] ** 2) - np.log(np.sqrt(2 * np.pi))\n    return logp\n\n# Posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\nn_steps = 11000\nburn_in = 1000\ndim = 4\nbeta_current = np.zeros(dim)\nlog_post_current = log_posterior(beta_current)\nsamples = np.zeros((n_steps, dim))\n\n# Proposal SDs\nproposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\nnp.random.seed(42)\n\nfor step in range(n_steps):\n    proposal = beta_current + np.random.normal(0, proposal_sds)\n    log_post_proposal = log_posterior(proposal)\n    accept_prob = np.exp(log_post_proposal - log_post_current)\n    if np.random.rand() &lt; accept_prob:\n        beta_current = proposal\n        log_post_current = log_post_proposal\n    samples[step] = beta_current\n\nposterior_samples = samples[burn_in:]\nposterior_samples\n\narray([[ 1.05609013,  0.61286261, -0.66019234, -0.09823814],\n       [ 0.98664628,  0.62786546, -0.67794842, -0.09616401],\n       [ 1.05266698,  0.58414367, -0.64656811, -0.09756155],\n       ...,\n       [ 1.30649992,  0.67801126, -0.90835072, -0.0988359 ],\n       [ 1.30649992,  0.67801126, -0.90835072, -0.0988359 ],\n       [ 1.30649992,  0.67801126, -0.90835072, -0.0988359 ]])\n\n\n\n\n\n\n\n\n\n\nColumn Index\nParameter Name\nInterpretation\n\n\n\n\n[ , 0]\nbeta_Netflix\nUser preference for Netflix, relative to the reference category (Hulu)\n\n\n[ , 1]\nbeta_Prime\nUser preference for Amazon Prime\n\n\n[ , 2]\nbeta_Ads\nNegative impact of ads (a more negative value indicates greater dislike for ads)\n\n\n[ , 3]\nbeta_Price\nNegative impact of price (a more negative value indicates greater sensitivity to price)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrace Plot (Top):\nThe trace plot shows rapid mixing and no obvious trend or drift, indicating that the sampler has converged and is exploring the posterior effectively.\nPosterior Histogram (Bottom):\nThe histogram approximates a smooth, bell-shaped distribution centered around the posterior mean. This reflects a stable estimate with symmetric uncertainty.\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbeta_Netflix\n1.040854\n0.116817\n0.814826\n1.281594\n\n\n1\nbeta_Prime\n0.589986\n0.116535\n0.365248\n0.839191\n\n\n2\nbeta_Ads\n-0.809980\n0.089848\n-0.983910\n-0.629427\n\n\n3\nbeta_Price\n-0.097349\n0.005689\n-0.108413\n-0.086486\n\n\n\n\n\n\n\n\nInterpretation\n\nbeta_Netflix and beta_Prime are positive, showing that people prefer Netflix and Prime over the base brand (Hulu).\nbeta_Ads is negative, meaning people dislike ads.\nbeta_Price is also negative, meaning higher prices reduce utility.\nAll 95% credible intervals do not include 0, so all four parameters are statistically meaningful. This confirms that the Bayesian estimation gives results consistent with what we expect based on the data."
  },
  {
    "objectID": "projects/project4/hw3_questions.html#discussion",
    "href": "projects/project4/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we did not simulate the data and instead received these results from a real-world conjoint study. Here’s what we can conclude:\n\nβ_Netflix &gt; β_Prime means that, on average, respondents prefer Netflix over Amazon Prime, all else being equal.\nThe positive values for both brand coefficients (compared to the baseline Hulu) suggest that both Netflix and Prime are generally preferred brands.\nβ_price is negative, which makes intuitive sense — higher prices reduce the likelihood of choosing an alternative. This is consistent with basic economic theory and consumer behavior.\nOverall, the signs and magnitudes of the parameters are reasonable and aligned with real-world expectations, even without knowing the data were simulated.\n\n\nWhy move beyond the basic MNL model?\nThe standard Multinomial Logit (MNL) model assumes that everyone shares the same preferences. For example:\n\nEveryone is equally likely to prefer Netflix over Hulu,\nand equally sensitive to price changes.\n\nBut in the real world, this is rarely true:\n\nSome people love Netflix, others don’t care.\nSome people are very price-sensitive, others are not.\nSome tolerate ads, others strongly avoid them.\n\nTo reflect this real-world variation, we need a model that allows individual-level differences in preferences. This leads us to the multi-level (or hierarchical) model.\nInstead of estimating just one set of parameters for the entire population, we assume each person ( i ) has their own preference vector ( _i ), drawn from a population distribution: [ _i (, ) ]\nThis model captures both overall trends and individual differences, making it much more realistic for analyzing real-world conjoint data.\n\n\nKey Modifications:\n\nSimulating hierarchical data:\n\nInstead of one global β vector, we assume each respondent ( i ) has their own preference vector ( _i ).\nThese individual ( _i )s are drawn from a population distribution: [ _i (, ) ]\nYou first draw ( _i ) for each respondent, and then simulate choices using that individual’s preferences.\n\nEstimating the model:\n\nYou no longer estimate a single ( ), but instead estimate:\n\n( ): the mean of the population distribution,\n( ): the covariance matrix (individual-level variation),\n( _i ): each respondent’s specific preferences (if using full Bayesian methods).\n\nThis often requires Bayesian methods (e.g., MCMC, HMC in PyMC or Stan) or frequentist approaches like simulated maximum likelihood."
  }
]